# Tests for mixture of normals factor models

# Helper function to simulate data from a 2-factor mixture model
simulate_2factor_mixture_data <- function(n_obs = 500, n_mix = 2,
                                           mix_weights = NULL, mix_means = NULL, mix_vars = NULL,
                                           seed = 123) {
  set.seed(seed)

  n_fac <- 2

  # Default mixture parameters (per factor)
  if (is.null(mix_weights)) {
    mix_weights <- rep(1/n_mix, n_mix)
  }
  if (is.null(mix_means)) {
    if (n_mix == 2) {
      # E[f] = 0 constraint per factor
      mix_means <- matrix(c(0.5, -mix_weights[1] * 0.5 / mix_weights[2],
                            0.3, -mix_weights[1] * 0.3 / mix_weights[2]),
                          nrow = n_mix, ncol = n_fac)
    } else {
      mix_means <- matrix(0, nrow = n_mix, ncol = n_fac)
    }
  }
  if (is.null(mix_vars)) {
    mix_vars <- matrix(c(0.8, 1.2, 0.9, 1.1), nrow = n_mix, ncol = n_fac)
  }

  # Sample mixture component for each observation
  mix_assign <- sample(1:n_mix, n_obs, replace = TRUE, prob = mix_weights)

  # Sample factors from mixture
  factors <- matrix(NA, nrow = n_obs, ncol = n_fac)
  for (i in 1:n_obs) {
    m <- mix_assign[i]
    for (k in 1:n_fac) {
      factors[i, k] <- rnorm(1, mean = mix_means[m, k], sd = sqrt(mix_vars[m, k]))
    }
  }

  # Generate 3 linear measurements per factor (6 total)
  loadings_f1 <- c(1.0, 0.8, 0.7)  # First loading fixed at 1
  loadings_f2 <- c(1.0, 0.6, 0.9)  # First loading fixed at 1
  sigma_meas <- 0.5

  Y <- matrix(NA, nrow = n_obs, ncol = 6)
  # Factor 1 measurements
  for (j in 1:3) {
    Y[, j] <- loadings_f1[j] * factors[, 1] + rnorm(n_obs, 0, sigma_meas)
  }
  # Factor 2 measurements
  for (j in 1:3) {
    Y[, j + 3] <- loadings_f2[j] * factors[, 2] + rnorm(n_obs, 0, sigma_meas)
  }
  colnames(Y) <- c("Y1_1", "Y1_2", "Y1_3", "Y2_1", "Y2_2", "Y2_3")

  # Generate outcomes of each type
  # Linear outcome
  Y_lin <- 0.5 + 0.6 * factors[, 1] + 0.4 * factors[, 2] + rnorm(n_obs, 0, 0.5)

  # Probit outcome
  latent_prob <- 0.3 * factors[, 1] + 0.5 * factors[, 2]
  Y_probit <- rbinom(n_obs, 1, pnorm(latent_prob))

  # Logit outcome (1/2 coded for C++ compatibility)
  latent_logit <- 0.4 * factors[, 1] + 0.3 * factors[, 2]
  Y_logit <- rbinom(n_obs, 1, 1 / (1 + exp(-latent_logit))) + 1  # 1 or 2 (not 0/1)

  # Ordered probit outcome (4 categories)
  latent_oprob <- 0.5 * factors[, 1] + 0.4 * factors[, 2] + rnorm(n_obs, 0, 0.5)
  Y_oprobit <- cut(latent_oprob, breaks = c(-Inf, -0.5, 0, 0.5, Inf), labels = FALSE)

  data <- as.data.frame(Y)
  data$Y_lin <- Y_lin
  data$Y_probit <- Y_probit
  data$Y_logit <- Y_logit
  data$Y_oprobit <- Y_oprobit
  data$intercept <- 1
  data$eval <- 1

  list(
    data = data,
    factors = factors,
    mix_assign = mix_assign,
    true_params = list(
      mix_weights = mix_weights,
      mix_means = mix_means,
      mix_vars = mix_vars
    )
  )
}


test_that("Mixture model with 2 components initializes correctly", {
  fm <- define_factor_model(n_factors = 1, n_mixtures = 2)
  expect_equal(fm$n_mixtures, 2L)
  # Expected params: 2 variances + 1 mean + 1 logweight = 4
  expect_equal(fm$nfac_param, 4L)
})


test_that("Mixture models cannot be combined with factor_mean_covariates", {
  expect_error(
    define_model_system(
      components = list(),
      factor = define_factor_model(n_factors = 1, n_mixtures = 2, factor_covariates = "x")
    ),
    "mutually exclusive"
  )
})


test_that("2-mixture model gradient is accurate (2 factors, linear)", {
  skip_on_cran()

  # Test with linear models only - probit/logit/oprobit have initialization issues with 2-factor mixtures
  sim <- simulate_2factor_mixture_data(n_obs = 100, n_mix = 2, seed = 42)
  dat <- sim$data

  fm <- define_factor_model(n_factors = 2, n_mixtures = 2)

  # Linear measurements for identification
  mc1 <- define_model_component("Y1_1", dat, "Y1_1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(1.0, 0), evaluation_indicator = "eval")
  mc2 <- define_model_component("Y1_2", dat, "Y1_2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(NA_real_, 0), evaluation_indicator = "eval")
  mc3 <- define_model_component("Y2_1", dat, "Y2_1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, 1.0), evaluation_indicator = "eval")
  mc4 <- define_model_component("Y2_2", dat, "Y2_2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, NA_real_), evaluation_indicator = "eval")

  ms <- define_model_system(components = list(mc1, mc2, mc3, mc4), factor = fm)

  # Initialize
  init <- initialize_parameters(ms, dat, verbose = FALSE)
  params <- init$init_params

  # Create C++ model
  fm_ptr <- initialize_factor_model_cpp(ms, dat, n_quad = 8)
  free_params <- extract_free_params_cpp(fm_ptr, params)

  # Compute analytical gradient
  result <- evaluate_likelihood_cpp(fm_ptr, free_params, compute_gradient = TRUE)

  # Compute finite-difference gradient
  eps <- 1e-6
  fd_grad <- numeric(length(free_params))
  for (i in seq_along(free_params)) {
    params_plus <- free_params
    params_plus[i] <- params_plus[i] + eps
    ll_plus <- evaluate_likelihood_cpp(fm_ptr, params_plus)$logLikelihood

    params_minus <- free_params
    params_minus[i] <- params_minus[i] - eps
    ll_minus <- evaluate_likelihood_cpp(fm_ptr, params_minus)$logLikelihood

    fd_grad[i] <- (ll_plus - ll_minus) / (2 * eps)
  }

  # Check gradient accuracy
  rel_err <- abs(result$gradient - fd_grad) / (abs(fd_grad) + 1e-8)
  max_rel_err <- max(rel_err)

  expect_lt(max_rel_err, 1e-4, label = paste("Max gradient relative error:", max_rel_err))
})


test_that("2-mixture model Hessian is accurate (2 factors, linear, ALL elements)", {
  skip_on_cran()

  # Test with linear models only - probit/logit/oprobit have initialization issues with 2-factor mixtures
  sim <- simulate_2factor_mixture_data(n_obs = 100, n_mix = 2, seed = 43)
  dat <- sim$data

  fm <- define_factor_model(n_factors = 2, n_mixtures = 2)

  # Linear measurements for identification
  mc1 <- define_model_component("Y1_1", dat, "Y1_1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(1.0, 0), evaluation_indicator = "eval")
  mc2 <- define_model_component("Y1_2", dat, "Y1_2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(NA_real_, 0), evaluation_indicator = "eval")
  mc3 <- define_model_component("Y2_1", dat, "Y2_1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, 1.0), evaluation_indicator = "eval")
  mc4 <- define_model_component("Y2_2", dat, "Y2_2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, NA_real_), evaluation_indicator = "eval")

  ms <- define_model_system(components = list(mc1, mc2, mc3, mc4), factor = fm)

  # Initialize
  init <- initialize_parameters(ms, dat, verbose = FALSE)
  params <- init$init_params

  # Create C++ model
  fm_ptr <- initialize_factor_model_cpp(ms, dat, n_quad = 8)
  free_params <- extract_free_params_cpp(fm_ptr, params)

  # Compute analytical Hessian
  result <- evaluate_likelihood_cpp(fm_ptr, free_params, compute_gradient = TRUE, compute_hessian = TRUE)

  # Convert upper-triangle to full matrix
  n_free <- length(free_params)
  hess_full <- matrix(0, n_free, n_free)
  idx <- 1
  for (i in 1:n_free) {
    for (j in i:n_free) {
      hess_full[i, j] <- result$hessian[idx]
      hess_full[j, i] <- result$hessian[idx]
      idx <- idx + 1
    }
  }

  # Compute finite-difference Hessian (full matrix)
  eps <- 1e-5
  fd_hess <- matrix(0, n_free, n_free)
  for (i in seq_along(free_params)) {
    params_plus <- free_params
    params_plus[i] <- params_plus[i] + eps
    grad_plus <- evaluate_likelihood_cpp(fm_ptr, params_plus, compute_gradient = TRUE)$gradient

    params_minus <- free_params
    params_minus[i] <- params_minus[i] - eps
    grad_minus <- evaluate_likelihood_cpp(fm_ptr, params_minus, compute_gradient = TRUE)$gradient

    fd_hess[i, ] <- (grad_plus - grad_minus) / (2 * eps)
  }
  fd_hess <- (fd_hess + t(fd_hess)) / 2

  # Check ALL upper triangle elements
  rel_errs <- numeric(0)
  for (i in 1:n_free) {
    for (j in i:n_free) {
      rel_err <- abs(hess_full[i, j] - fd_hess[i, j]) / (abs(fd_hess[i, j]) + 1e-8)
      rel_errs <- c(rel_errs, rel_err)
    }
  }

  # Check Hessian accuracy - all elements should be accurate
  expect_lt(median(rel_errs), 0.01, label = sprintf("Median Hessian error: %.4f", median(rel_errs)))
  expect_lt(quantile(rel_errs, 0.9), 0.1, label = sprintf("90th percentile Hessian error: %.4f", quantile(rel_errs, 0.9)))
})


test_that("SE_linear model with 2-mixture input factor works", {
  skip_on_cran()

  set.seed(45)
  n_obs <- 500

  # Simulate from mixture on input factor (f1)
  mix_weights <- c(0.6, 0.4)
  mix_vars_f1 <- c(0.8, 1.2)
  mix_mean1_f1 <- 0.5
  mix_mean2_f1 <- -mix_weights[1] * mix_mean1_f1 / mix_weights[2]

  mix_assign <- sample(1:2, n_obs, replace = TRUE, prob = mix_weights)

  f1 <- numeric(n_obs)
  for (i in 1:n_obs) {
    m <- mix_assign[i]
    f1[i] <- rnorm(1, mean = c(mix_mean1_f1, mix_mean2_f1)[m], sd = sqrt(mix_vars_f1[m]))
  }

  # SE_linear: f2 = alpha + alpha_1 * f1 + epsilon
  se_intercept <- 0.5
  se_linear <- 0.6
  se_residual_var <- 0.5
  f2 <- se_intercept + se_linear * f1 + rnorm(n_obs, 0, sqrt(se_residual_var))

  Y1 <- 1.0 * f1 + rnorm(n_obs, 0, 0.5)
  Y2 <- 0.8 * f1 + rnorm(n_obs, 0, 0.5)
  Y3 <- 1.0 * f2 + rnorm(n_obs, 0, 0.5)
  Y4 <- 0.7 * f2 + rnorm(n_obs, 0, 0.5)

  dat <- data.frame(Y1 = Y1, Y2 = Y2, Y3 = Y3, Y4 = Y4, intercept = 1, eval = 1)

  fm <- define_factor_model(n_factors = 2, n_mixtures = 2, factor_structure = "SE_linear")

  mc1 <- define_model_component("Y1", dat, "Y1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(1.0, 0), evaluation_indicator = "eval")
  mc2 <- define_model_component("Y2", dat, "Y2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(NA_real_, 0), evaluation_indicator = "eval")
  mc3 <- define_model_component("Y3", dat, "Y3", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, 1.0), evaluation_indicator = "eval")
  mc4 <- define_model_component("Y4", dat, "Y4", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, NA_real_), evaluation_indicator = "eval")

  ms <- define_model_system(components = list(mc1, mc2, mc3, mc4), factor = fm)
  control <- define_estimation_control(n_quad_points = 12)

  # Check parameter names
  init <- initialize_parameters(ms, dat, verbose = FALSE)
  param_names <- names(init$init_params)

  expect_true("mix1_factor_var_1" %in% param_names, label = "mix1_factor_var_1 present")
  expect_true("mix2_factor_var_1" %in% param_names, label = "mix2_factor_var_1 present")
  expect_true("mix1_factor_mean_1" %in% param_names, label = "mix1_factor_mean_1 present")
  expect_true("mix1_logweight" %in% param_names, label = "mix1_logweight present")
  expect_true("se_intercept" %in% param_names, label = "se_intercept present")
  expect_true("se_linear_1" %in% param_names, label = "se_linear_1 present")
  expect_true("se_residual_var" %in% param_names, label = "se_residual_var present")

  result <- estimate_model_rcpp(model_system = ms, data = dat, control = control, optimizer = "nlminb", verbose = FALSE)

  expect_true(result$convergence %in% c(0, 1), label = paste("SE_linear+mixture convergence:", result$convergence))

  est <- result$estimates
  expect_true(is.finite(est["se_intercept"]), label = "se_intercept is finite")
  expect_true(is.finite(est["se_linear_1"]), label = "se_linear_1 is finite")
  expect_true(is.finite(est["se_residual_var"]) && est["se_residual_var"] > 0, label = "se_residual_var is positive")
})


test_that("SE_quadratic model with 2-mixture input factor works", {
  skip_on_cran()

  set.seed(49)
  n_obs <- 500

  mix_weights <- c(0.6, 0.4)
  mix_vars_f1 <- c(0.8, 1.2)
  mix_mean1_f1 <- 0.5
  mix_mean2_f1 <- -mix_weights[1] * mix_mean1_f1 / mix_weights[2]

  mix_assign <- sample(1:2, n_obs, replace = TRUE, prob = mix_weights)

  f1 <- numeric(n_obs)
  for (i in 1:n_obs) {
    m <- mix_assign[i]
    f1[i] <- rnorm(1, mean = c(mix_mean1_f1, mix_mean2_f1)[m], sd = sqrt(mix_vars_f1[m]))
  }

  # SE_quadratic: f2 = alpha + alpha_1 * f1 + alpha_q1 * f1^2 + epsilon
  se_intercept <- 0.5
  se_linear <- 0.6
  se_quadratic <- 0.2
  se_residual_var <- 0.5
  f2 <- se_intercept + se_linear * f1 + se_quadratic * f1^2 + rnorm(n_obs, 0, sqrt(se_residual_var))

  Y1 <- 1.0 * f1 + rnorm(n_obs, 0, 0.5)
  Y2 <- 0.8 * f1 + rnorm(n_obs, 0, 0.5)
  Y3 <- 1.0 * f2 + rnorm(n_obs, 0, 0.5)
  Y4 <- 0.7 * f2 + rnorm(n_obs, 0, 0.5)

  dat <- data.frame(Y1 = Y1, Y2 = Y2, Y3 = Y3, Y4 = Y4, intercept = 1, eval = 1)

  fm <- define_factor_model(n_factors = 2, n_mixtures = 2, factor_structure = "SE_quadratic")

  mc1 <- define_model_component("Y1", dat, "Y1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(1.0, 0), evaluation_indicator = "eval")
  mc2 <- define_model_component("Y2", dat, "Y2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(NA_real_, 0), evaluation_indicator = "eval")
  mc3 <- define_model_component("Y3", dat, "Y3", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, 1.0), evaluation_indicator = "eval")
  mc4 <- define_model_component("Y4", dat, "Y4", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = c(0, NA_real_), evaluation_indicator = "eval")

  ms <- define_model_system(components = list(mc1, mc2, mc3, mc4), factor = fm)
  control <- define_estimation_control(n_quad_points = 12)

  # Check parameter names
  init <- initialize_parameters(ms, dat, verbose = FALSE)
  param_names <- names(init$init_params)

  expect_true("mix1_factor_var_1" %in% param_names, label = "mix1_factor_var_1 present (SE_quad)")
  expect_true("se_quadratic_1" %in% param_names, label = "se_quadratic_1 present (SE_quad)")

  result <- estimate_model_rcpp(model_system = ms, data = dat, control = control, optimizer = "nlminb", verbose = FALSE)

  expect_true(result$convergence %in% c(0, 1), label = paste("SE_quadratic+mixture convergence:", result$convergence))

  est <- result$estimates
  expect_true(is.finite(est["se_quadratic_1"]), label = "se_quadratic_1 is finite")
  expect_true(is.finite(est["se_residual_var"]) && est["se_residual_var"] > 0, label = "se_residual_var is positive (SE_quad)")
  expect_true(is.finite(est["mix1_factor_var_1"]) && est["mix1_factor_var_1"] > 0, label = "mix1_factor_var_1 is positive (SE_quad)")
})


test_that("nmix=3 model initializes correctly", {
  fm <- define_factor_model(n_factors = 1, n_mixtures = 3)
  expect_equal(fm$n_mixtures, 3L)
  # Expected params: 3 variances + 2 means + 2 logweights = 7
  expect_equal(fm$nfac_param, 7L)
})


test_that("nmix > 3 is not allowed", {
  expect_error(
    define_factor_model(n_factors = 1, n_mixtures = 4),
    "should be between"
  )
})


test_that("2-mixture 2-factor model initializes correctly", {
  fm <- define_factor_model(n_factors = 2, n_mixtures = 2)
  expect_equal(fm$n_mixtures, 2L)
  expect_equal(fm$n_factors, 2L)
  # Expected params: 2*2 variances + 1*2 means + 1 logweight = 7
  expect_equal(fm$nfac_param, 7L)
})


test_that("2-mixture 1-factor model recovers parameters with well-separated mixtures", {
  skip_on_cran()

  # Design for good mixture identification:
  # - 1 factor (simpler model)
  # - 4 linear measures (good factor identification)
  # - 4-sigma mean separation (mixtures are well-separated)
  # - Equal variances (avoids variance-mean trade-off)
  # - N=1500 for statistical power
  set.seed(53)
  n_obs <- 1500

  # True parameters
  mix_weights <- c(0.5, 0.5)
  mix_vars <- c(1.0, 1.0)      # Equal variances
  mix_means <- c(2.0, -2.0)    # 4-sigma separation
  loadings <- c(1.0, 0.8, 0.7, 0.9)
  sigma_meas <- 0.25

  # Sample mixture component
  mix_assign <- sample(1:2, n_obs, replace = TRUE, prob = mix_weights)

  # Sample factors
  factors <- numeric(n_obs)
  for (i in 1:n_obs) {
    m <- mix_assign[i]
    factors[i] <- rnorm(1, mean = mix_means[m], sd = sqrt(mix_vars[m]))
  }

  # Generate 4 linear measurements
  Y <- matrix(NA, nrow = n_obs, ncol = 4)
  for (j in 1:4) {
    Y[, j] <- loadings[j] * factors + rnorm(n_obs, 0, sigma_meas)
  }
  colnames(Y) <- c("Y1", "Y2", "Y3", "Y4")

  dat <- as.data.frame(Y)
  dat$intercept <- 1
  dat$eval <- 1

  # Define 1-factor 2-mixture model
  fm <- define_factor_model(n_factors = 1, n_mixtures = 2)

  mc1 <- define_model_component("Y1", dat, "Y1", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = 1.0, evaluation_indicator = "eval")
  mc2 <- define_model_component("Y2", dat, "Y2", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = NA_real_, evaluation_indicator = "eval")
  mc3 <- define_model_component("Y3", dat, "Y3", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = NA_real_, evaluation_indicator = "eval")
  mc4 <- define_model_component("Y4", dat, "Y4", fm, covariates = "intercept", model_type = "linear",
                                 loading_normalization = NA_real_, evaluation_indicator = "eval")

  ms <- define_model_system(components = list(mc1, mc2, mc3, mc4), factor = fm)
  control <- define_estimation_control(n_quad_points = 16, num_cores = 4)

  # Estimate with parallelization for speed
  result <- estimate_model_rcpp(model_system = ms, data = dat, control = control,
                                 optimizer = "nlminb", parallel = TRUE, verbose = FALSE)

  # Check convergence
  expect_equal(result$convergence, 0, label = "Convergence must be 0 for parameter recovery test")

  est <- result$estimates
  se <- result$std_errors

  # Compute derived parameters
  est_w1 <- exp(est["mix1_logweight"]) / (1 + exp(est["mix1_logweight"]))
  est_mix2_mean <- -est_w1 * est["mix1_factor_mean_1"] / (1 - est_w1)

  # Helper function to check parameter within tolerance SEs
  check_param <- function(param_name, true_value, tolerance_se = 3.0) {
    if (!(param_name %in% names(est))) {
      fail(paste("Parameter", param_name, "not found in estimates"))
      return()
    }
    estimate <- est[param_name]
    std_err <- se[param_name]

    # Validate SE is reasonable
    expect_true(is.finite(std_err) && std_err > 0 && std_err < 5,
                label = paste(param_name, "SE is reasonable:", round(std_err, 4)))

    # Check within tolerance_se standard errors
    z_score <- abs(estimate - true_value) / std_err
    expect_lt(z_score, tolerance_se,
              label = sprintf("%s: est=%.3f, true=%.3f, SE=%.3f, z=%.2f",
                              param_name, estimate, true_value, std_err, z_score))
  }

  # Check measurement loadings (well-identified, should be within 2.5 SEs)
  check_param("Y2_loading_1", loadings[2], tolerance_se = 2.5)
  check_param("Y3_loading_1", loadings[3], tolerance_se = 2.5)
  check_param("Y4_loading_1", loadings[4], tolerance_se = 2.5)

  # Check mixture means (well-identified with 4-sigma separation)
  check_param("mix1_factor_mean_1", mix_means[1], tolerance_se = 3.0)

  # Check derived mix2 mean (should satisfy E[f]=0 constraint)
  # Use relative error since we don't have SE for derived parameter
  rel_err_mix2_mean <- abs(est_mix2_mean - mix_means[2]) / abs(mix_means[2])
  expect_lt(rel_err_mix2_mean, 0.15,
            label = sprintf("mix2_factor_mean_1: est=%.3f, true=%.3f, rel_err=%.1f%%",
                            est_mix2_mean, mix_means[2], rel_err_mix2_mean * 100))

  # Check mixture weight (well-identified with separation)
  # Use relative error for derived parameter
  rel_err_weight <- abs(est_w1 - mix_weights[1]) / mix_weights[1]
  expect_lt(rel_err_weight, 0.15,
            label = sprintf("weight_1: est=%.3f, true=%.3f, rel_err=%.1f%%",
                            est_w1, mix_weights[1], rel_err_weight * 100))

  # Mixture variances are harder to identify (known trade-off with measurement error)
  # The within-component variances may be biased, but check they are positive and finite
  expect_true(est["mix1_factor_var_1"] > 0 && is.finite(est["mix1_factor_var_1"]))
  expect_true(est["mix2_factor_var_1"] > 0 && is.finite(est["mix2_factor_var_1"]))

  # Both variances should be similar (within 50% of each other) since true values are equal
  var_ratio <- est["mix1_factor_var_1"] / est["mix2_factor_var_1"]
  expect_true(var_ratio > 0.5 && var_ratio < 2.0,
              label = sprintf("Variance ratio: %.2f (should be ~1.0 for equal true variances)", var_ratio))

  # Check OVERALL/MARGINAL variance of the factor distribution
  # Var(f) = E[Var(f|m)] + Var(E[f|m]) = sum(w_m * var_m) + sum(w_m * mean_m^2)
  # The within-component variance may be poorly identified, but the total variance
  # should be reasonably accurate because the between-component variance (from means)
  # is well-identified and compensates.
  est_w2 <- 1 - est_w1
  est_weights <- c(est_w1, est_w2)
  est_vars <- c(est["mix1_factor_var_1"], est["mix2_factor_var_1"])
  est_means <- c(est["mix1_factor_mean_1"], est_mix2_mean)

  # True total variance
  true_within_var <- sum(mix_weights * mix_vars)
  true_between_var <- sum(mix_weights * mix_means^2)
  true_total_var <- true_within_var + true_between_var

  # Estimated total variance
  est_within_var <- sum(est_weights * est_vars)
  est_between_var <- sum(est_weights * est_means^2)
  est_total_var <- est_within_var + est_between_var

  # Total variance should be within 15% of true value
  rel_err_total_var <- abs(est_total_var - true_total_var) / true_total_var
  expect_lt(rel_err_total_var, 0.15,
            label = sprintf("Total factor variance: est=%.3f, true=%.3f, rel_err=%.1f%%",
                            est_total_var, true_total_var, rel_err_total_var * 100))

  # Between-component variance (from means) should be well-identified
  rel_err_between_var <- abs(est_between_var - true_between_var) / true_between_var
  expect_lt(rel_err_between_var, 0.15,
            label = sprintf("Between-component variance: est=%.3f, true=%.3f, rel_err=%.1f%%",
                            est_between_var, true_between_var, rel_err_between_var * 100))
})
